{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "Lab-7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "MEA8CG5vWQnb"
      },
      "source": [
        "\n",
        "# Lab-7:\n",
        "In this lab, we will examine some classifiers and the regularization concept in the classification problem.\n",
        "Also, we will see\n",
        "\n",
        "### Objectives:\n",
        "1. Lasso and Ridge\n",
        "2. Regularization in Keras.\n",
        "2. PCA\n",
        "\n",
        "---\n",
        "\n",
        "## Lasso and Ridge\n",
        "Both models are the regularized forms of the linear regression.\n",
        "Lass with L1 regularization and Ridge with L2 regularization.\n",
        "Both act as a constraint region for the coeffeicients/weight, where they must reside in.\n",
        "\n",
        "### Issues:\n",
        "1. When to use Lasso?\n",
        "2. When to use Ridge?\n",
        "3. Since it is hard to decide the parameters influence, How we can decide which regularization? and decide the value of lambda?\n",
        "\n",
        "\n",
        "### Loading Boston dataset\n",
        "Housing-Prices Values in Suburbs of Boston."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "qjUZVz7VWQnd"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_boston(return_X_y=True)\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=1/8, random_state=123)\n",
        "print(x_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "zCr51wZGWQne"
      },
      "source": [
        "### Fitting both Lasso and Ridge\n",
        "Task:\n",
        "\n",
        "Fit two models: Lasso and Ridge - with the default alpha-.\n",
        "Then print their coefficients and notice the difference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "4B5kprDUWQne"
      },
      "source": [
        "from sklearn.linear_model import Lasso, Ridge\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "UlA2KnJEWQnf"
      },
      "source": [
        "Task: Let's try different values for alpha for Lasso regressor and plot the validation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "8jS83H8RWQnf"
      },
      "source": [
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "%matplotlib inline\n",
        "\n",
        "alphas = [2.2, 2, 1.5, 1.3, 1.2, 1.1, 1, 0.3, 0.1]\n",
        "losses = []\n",
        "for alpha in alphas:\n",
        "    # Write (5 lines): create a Lasso regressor with the alpha value.\n",
        "    # Fit it to the training set, then get the prediction of the validation set (x_val).\n",
        "    # calculate the mean sqaured error loss, then append it to the losses array\n",
        "    \n",
        "plt.plot(alphas, losses)\n",
        "plt.xlabel(\"alpha\")\n",
        "plt.ylabel(\"Mean squared error\")\n",
        "plt.show()\n",
        "\n",
        "best_alpha = alphas[np.argmin(losses)]\n",
        "print(\"Best value of alpha:\", best_alpha)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "ae1rr2bNWQng"
      },
      "source": [
        "Measuring the loss on the testset with Lasso regressor with the best alpha."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "0zrYXIW7WQng"
      },
      "source": [
        "lasso = Lasso(best_alpha)\n",
        "lasso.fit(x_train, y_train)\n",
        "y_pred = lasso.predict(x_test)\n",
        "print(\"MSE on testset:\", mean_squared_error(y_test, y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPU_UatoXhQN"
      },
      "source": [
        "### How to Do it in Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0n7FL614YZVY"
      },
      "source": [
        "Task: add regularization in the dense layers in the following model, with "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FykodPiXoHh"
      },
      "source": [
        "#%%\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras import regularizers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from math import sqrt, ceil\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data() \n",
        "train_shape = x_train.shape\n",
        "test_shape = x_test.shape\n",
        "print(x_train.shape, x_test.shape)\n",
        "#Images are 2D. What's the difference in 3D images?\n",
        "x_train = x_train.reshape(train_shape[0], train_shape[1] * train_shape[2])\n",
        "x_test = x_test.reshape(test_shape[0], test_shape[1] * test_shape[2])\n",
        "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)\n",
        "\n",
        "\n",
        "\n",
        "# Some hyperparamters:\n",
        "\n",
        "\n",
        "epochs = 5\n",
        "batch_size = 10\n",
        "input_size = x_train.shape[1]\n",
        "num_classes = 10 \n",
        "\n",
        "# Define your regularizer here (1 line).\n",
        "\n",
        "def get_model():\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(units=128,\n",
        "                    input_dim=input_size,\n",
        "                    activation='sigmoid'))\n",
        "\n",
        "    # Add 2 hidden layers with number of units: 32, 64 \n",
        "\n",
        "\n",
        "    model.add(Dense(units=num_classes, use_bias=True, activation='softmax'))\n",
        "    #Try to change the optimizer, visit: https://goo.gl/dHFJNy\n",
        "    #Try to change the loss func, visit: https://goo.gl/xMrooU\n",
        "    #Try to change learning rate (lr)\n",
        "    #In your free time take a look at different variations of GD: https://goo.gl/YFa6XY\n",
        "    sgd_optimizer = SGD(lr=.01)\n",
        "    adam_optimizer = Adam(lr=.001)\n",
        "    model.compile(optimizer=sgd_optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model = get_model()\n",
        "model.fit(x=x_train, y=y_train, batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "loss, acc= model.evaluate(x_test, y_test)\n",
        "print(\"Loss:\", loss, \", Accuracy:\", acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "pycharm": {
          "name": "#%% md\n"
        },
        "id": "zgeDfUGOWQnh"
      },
      "source": [
        "## PCA\n",
        "\n",
        "1. How does PCA reduce data dimensionality?\n",
        "2. What is eigenvector?\n",
        "\n",
        "Task: Now you will implement basic steps of PCA: mean-centering, eigenvectors calculation using covariance matrix, projecting data to the first PC, and restoring it back."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ns8LzkluWQnh"
      },
      "source": [
        "### Generating data ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "-e7HmptwWQni"
      },
      "source": [
        "# N is a sample size\n",
        "N = 25\n",
        "# we can fix a random seed. It allows us to get the same data\n",
        "np.random.seed(10)\n",
        "# form our data\n",
        "x = np.linspace(-5, -3, N)\n",
        "y = 10 + 2*x + np.random.random(size=(N,))\n",
        "data = np.stack([x,y], axis = 1)\n",
        "\n",
        "\n",
        "plt.title(\"Data\")\n",
        "plt.plot(data[:,0], data[:,1], '.', color=\"green\")\n",
        "plt.axis([-6, 2, -3, 6])\n",
        "plt.grid('True')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "YtQw3dQtWQni"
      },
      "source": [
        "### Center data###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "AbQa4yj9WQnj"
      },
      "source": [
        "# center data by subtracting mean value from each feature\n",
        "# pay attention to mean_vector <-- we need it later for restoring our data\n",
        "mean_vector = ???\n",
        "data_centered = ???\n",
        "\n",
        "plt.title(\"Centered data\")\n",
        "plt.plot(data[:,0], data[:,1], '.', color=\"green\")\n",
        "plt.plot(data_centered[:,0], data_centered[:,1], '.', color=\"blue\")\n",
        "plt.axis([-6, 2, -3, 6])\n",
        "plt.grid('True')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "_9-LjVkqWQnj"
      },
      "source": [
        "### Covariance matrix ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "23QWMin6WQnj"
      },
      "source": [
        "# calculate covariance matrix for our centered data\n",
        "cov_mat = ???\n",
        "print('Covariance matrix:\\n', cov_mat)\n",
        "\n",
        "# Cov(x, y) = (1 / (n - 1)) * Sum_i(x_i * y_i)\n",
        "# also, to make sure you understand how to calculate covariance, calculate and print cov(X,Y)\n",
        "# check that it is the same as in the covariance matrix\n",
        "cov_xy = ???\n",
        "print('cov(X,Y):', cov_xy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "GLno4bPlWQnk"
      },
      "source": [
        "### Eigenvectors and eigenvalues"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "74Az2bqwWQnk"
      },
      "source": [
        "# compute eigenvectors and eigenvalues, print them\n",
        "eig_values, eig_vectors = ???\n",
        "print('eig_values:', eig_values)\n",
        "print('eig_vectors:\\n', eig_vectors)\n",
        "\n",
        "# are they already in the needed order?\n",
        "# if not, order eigenvectors and eigenvalues by eigenvalues, descending (3 lines)\n",
        "# Note: the eig_vectors is a col vectors.\n",
        "\n",
        "\n",
        "print('\\nsorteed eig_values:', eig_values)\n",
        "print('sorted eig_vectors:\\n', eig_vectors)\n",
        "\n",
        "# estimate variance retained by each principal component\n",
        "retained_var = eig_values / eig_values.sum()\n",
        "print('\\nretained variance:',   retained_var)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "TOs6hymtWQnk"
      },
      "source": [
        "### Project data ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "pSVR1uBeWQnl"
      },
      "source": [
        "# project data to the first principal component\n",
        "first_pc = ???\n",
        "projected_data = ???\n",
        "\n",
        "plt.title(\"Projected data\")\n",
        "plt.plot(data[:,0], data[:,1], '.', color=\"green\")\n",
        "plt.plot(data_centered[:,0], data_centered[:,1], '.', color=\"blue\")\n",
        "plt.plot(projected_data, np.zeros(len(projected_data)), '.', color=\"red\")\n",
        "plt.axis([-6, 3, -3, 6])\n",
        "plt.grid('True')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "zLi5AUrVWQnm"
      },
      "source": [
        "### Restore data back ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "bQw2yirXWQnm"
      },
      "source": [
        "# Projected_data . first_pc.T + means\n",
        "# project data back to initial space\n",
        "# remember to add a mean_vector to the restored data\n",
        "restored_data = ???\n",
        "\n",
        "plt.title(\"Restored data\")\n",
        "plt.plot(data[:,0], data[:,1], '.', color=\"green\")\n",
        "plt.plot(data_centered[:,0], data_centered[:,1], '.', color=\"blue\")\n",
        "plt.plot(restored_data[:,0], restored_data[:,1], '.', color=\"red\")\n",
        "plt.axis([-6, 2, -3, 6])\n",
        "plt.grid('True')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "-h4FLs6jWQnm"
      },
      "source": [
        "### SKLEARN implementation ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "itFqJZL3WQnm"
      },
      "source": [
        "# this is to check your solution\n",
        "from sklearn.decomposition import PCA\n",
        "pca = PCA(n_components=1)\n",
        "x_PCA = pca.fit_transform(data)\n",
        "\n",
        "plt.title(\"Projected data\")\n",
        "plt.plot(data[:,0], data[:,1], '.', color=\"green\")\n",
        "plt.plot(data_centered[:,0], data_centered[:,1], '.', color=\"blue\")\n",
        "plt.plot(x_PCA, np.zeros(len(projected_data)), '.', color=\"red\")\n",
        "plt.axis([-6, 3, -3, 6])\n",
        "plt.grid('True')\n",
        "\n",
        "print(pca.mean_)\n",
        "print(pca.components_)\n",
        "print(pca.explained_variance_)\n",
        "print(pca.explained_variance_ratio_)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}